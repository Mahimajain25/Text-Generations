# Text-Generations using GPT-Neo 


- GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.
- GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.
- This model was trained on the Pile for 380 billion tokens over 362,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.

# Demo
![text Generation](https://user-images.githubusercontent.com/96101074/206457491-fd76055e-d1af-4f0d-a5fa-66594a911e7a.png)
